{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPKTqQ3W8qLt"
   },
   "source": [
    "To begin upload this notebook to your own drive:\n",
    "\n",
    "### Submission Instructions:\n",
    "1. **Restart the kernel** (in the menubar, select Runtime$\\rightarrow$Restart runtime)\n",
    "2. **Run all cells** (in the menubar, select Runtime$\\rightarrow$Run All).\n",
    "3. **Download the notebook** (in the menubar, select File$\\rightarrow$Download .ipynb)\n",
    "4. **Upload the downloaded notebook (.ipynb file) to your repository**.\n",
    "\n",
    "Note: To use a GPU, do the following: Runtime$\\rightarrow$Change runtime type$\\rightarrow$ GPU\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE`, and that no tests fail.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XvPkt7Zm8qLt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5493400e8b7f9a8e2cde874866d4fa7f",
     "grade": false,
     "grade_id": "cell-3a1bca1dbb7d0069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ![shakespeare](https://i.imgur.com/81YZuel.jpg)\n",
    "\n",
    "# Generating Shakespeare Using a Character-level Language Model\n",
    "\n",
    "### From Words to Characters\n",
    "In the previous section we dealt with word-level language models. But looking again at section 1, there is nothing that constraints us to using _words_ as the basic elemnents in our model. The model we analyzed in section 1 could just as well be character-based - just replace \"word\" with \"character\", and you are good to go. In this notebook we will train a small character-based language model that will help us generate Shakespearean-like (emphasis on the _like_...) texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EvGyr_ux8qLt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af7a343d0e3524c3fd846d987d766a8",
     "grade": false,
     "grade_id": "cell-7301754e4d655d01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2.a\n",
    "Can you think of an advantage a character-based language model could have over a word-based language model? And what about the other way around: can you think of an advantage a word-based language model could have over a character-based language model? (Add your answer to the final submission pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ghCevRFf8qLt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d46a8dfd340b8f68e51a041307f7d7d3",
     "grade": false,
     "grade_id": "cell-ebc0d8ae3061c0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Using PyTorch\n",
    "\n",
    "We'll build our language model using PyTorch. PyTorch is a [very popular](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) open-source machine learning (and deep learning) framework developed by Facebook. In short:\n",
    "\n",
    "> Pytorch is a Python-based scientific computing package targeted at two sets of audiences:\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "To get familiar with PyTorch, check out this [quick tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html). In addition, another imporant difference from numpy is that PyTorch can automatically calculate the gradients needed for backpropagation, as explained [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MYd79g6k8qLt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02af8a21a2e8fae58d84f915de5b016d",
     "grade": false,
     "grade_id": "cell-aa2773db1bef7014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Our dataset is a plain text file. For simplicity, we turn any potential unicode characters into plain ASCII by using the `unidecode` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:00.336991Z",
     "start_time": "2025-05-18T13:16:59.063282Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OT5WdSbsFT1K",
    "outputId": "d7b21156-e469-463b-9ba9-b92e4cf6bf24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\danie\\miniconda3\\envs\\nlp-hw2\\lib\\site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:01.480975Z",
     "start_time": "2025-05-18T13:17:00.345659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "RYqFoQgV8qLt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef0359e8c08b2057771c115150011e7e",
     "grade": false,
     "grade_id": "cell-cce75419c097f3fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2726b5e9-17a7-4c51-f92c-fe6973828619",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in our dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "# import re\n",
    "import requests\n",
    "import unidecode\n",
    "url = \"https://github.com/tau-nlp-course/NLP_HW2/raw/main/data/shakespeare.txt\"\n",
    "\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)  # our vocabulary size (|V| from the handout)\n",
    "\n",
    "dataset_as_string = unidecode.unidecode(requests.get(url).content.decode())\n",
    "n_chars_in_dataset = len(dataset_as_string)\n",
    "print(f'Total number of characters in our dataset: {n_chars_in_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mIctyT3J8qLu",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06dd2ac91a6296206475c7e330e53e3d",
     "grade": false,
     "grade_id": "cell-d795f907dd7922f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To make inputs out of this big string of text, we will split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:01.639993Z",
     "start_time": "2025-05-18T13:17:01.635324Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "eoLs0ivz8qLu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61947ad22fb7f16eba246d47ab8cae22",
     "grade": false,
     "grade_id": "cell-379f229536dae19b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "7e36a18c-ecc8-4c37-db46-07018cb2266a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d.\n",
      "\n",
      "BUCKINGHAM:\n",
      "My Lord!\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, what's o'clock?\n",
      "\n",
      "BUCKINGHAM:\n",
      "I am thus bold to put your grace in mind\n",
      "Of what you promised me.\n",
      "\n",
      "KING RICHARD III:\n",
      "Well, but what's o'clock?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Upon the stroke of ten.\n",
      "\n",
      "KING RICHARD III:\n",
      "Well, let it strike.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Why let it strike?\n",
      "\n",
      "KING RICHARD III:\n",
      "Because that, like a Jack, thou keep'st the stroke\n",
      "Betwixt thy begging and my meditat\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 400\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, n_chars_in_dataset - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return dataset_as_string[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ho8WlUcV8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba5d4900ff254fa335fe935962878c8d",
     "grade": false,
     "grade_id": "cell-fcbb2d73f4e442fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Building Our Model\n",
    "\n",
    "Our model consists of three main components:\n",
    "\n",
    "1. [**Embedding**](https://pytorch.org/docs/stable/nn.html#embedding). A mapping between characters and their learned representations (\"word vectors\") \\[correspoding to ${\\boldsymbol L}$ in terms of the handout\\]\n",
    "2. [**GRU**](https://pytorch.org/docs/stable/nn.html#gru). \\[correspoding to the computation of ${\\boldsymbol h}^{(t)}$ in terms of the handout\\]\n",
    "3. **Output Layer**. A feed-forward neural network that transforms a hidden state at a timestep into a probability distribution of the next character. \\[correspoding to the computation of $\\hat{\\boldsymbol y}^{(t)}$ in terms of the handout\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFm8g2pd8qLv"
   },
   "source": [
    "### Question 2.b\n",
    "Complete the implementation of the `forward` method of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.306882Z",
     "start_time": "2025-05-18T13:17:01.653461Z"
    },
    "deletable": false,
    "id": "2SoCQ_ZM8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9ad1239fcd5aec23f439249397895ec",
     "grade": false,
     "grade_id": "cell-1640492438386e87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class OurModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size,\n",
    "                                      embedding_dim=hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        # General instructions:\n",
    "        # Pass the embedded input through the GRU and use the output layer to get the next character distribution.\n",
    "        # return that distribution and the next hidden state.\n",
    "        # You may need to play around with the dimensions a bit until you get it right. Dimension-induced frustration is good for you!\n",
    "        # -------------------------\n",
    "        # YOUR CODE HERE\n",
    "        if isinstance(input_, int):  # Python int\n",
    "            input_ = torch.tensor([[input_]], dtype=torch.long, device=next(self.parameters()).device)\n",
    "        elif isinstance(input_, torch.Tensor):\n",
    "            if input_.dim() == 0:\n",
    "                input_ = input_.view(1, 1)\n",
    "            elif input_.dim() == 1:\n",
    "                input_ = input_.view(1, -1)\n",
    "\n",
    "        embedded  = self.embedding(input_) # (1, batch, hidden_size)\n",
    "        output, hidden = self.gru(embedded, hidden) # (1, batch, hidden_size)\n",
    "        output = self.output_layer(output.squeeze(0)) # (batch, output_size)\n",
    "        # -------------------------\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.num_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aZWMbY1o8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da793a49917dc4882e7e70f04d07a777",
     "grade": false,
     "grade_id": "cell-b9299fddeb082b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Creating the Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B-ngTV6Q8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6eaeb80c370b32f26eda2ac1be57444",
     "grade": false,
     "grade_id": "cell-83bf9e1b0374206c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each chunk will be turned into a tensor by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.320746Z",
     "start_time": "2025-05-18T13:17:03.307909Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "98wyNtkw8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc87bca342db2fde1b3957f48bcfe857",
     "grade": false,
     "grade_id": "cell-5360afdd0b03b1f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "05623659-53d1-40b4-d75c-6432d5c23b90",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn a string into list of longs\n",
    "def chars_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(chars_to_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cYh_R1K88qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7fab2aa0d22a697fcc3d675b1821875",
     "grade": false,
     "grade_id": "cell-6e7b3d9e8c9396bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can assemble a pair of input and target tensors (i.e. a single training example) for training, from a random chunk. The input will be all characters *except the last*, and the target will be all characters *except the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.345169Z",
     "start_time": "2025-05-18T13:17:03.339716Z"
    },
    "deletable": false,
    "editable": false,
    "id": "QFDYhW3a8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adf90d7ec6728b2f45d1e8de5c47203c",
     "grade": false,
     "grade_id": "cell-d3539c5f1d96a188",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = chars_to_tensor(chunk[:-1])\n",
    "    target = chars_to_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eU6VTX8F8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18a6bf800d9bc590739b15ba01dda408",
     "grade": false,
     "grade_id": "cell-16d13f3b273395ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.369245Z",
     "start_time": "2025-05-18T13:17:03.362907Z"
    },
    "deletable": false,
    "editable": false,
    "id": "xeoACNc78qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a47c721a818979b886f119401206e756",
     "grade": false,
     "grade_id": "cell-44ab27a8fee696ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = model.init_hidden()\n",
    "    prime_input = chars_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist =  F.softmax(output / temperature, dim=-1)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = chars_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iNmsUvyM8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fffa10554299eaae14cc007fea3935a",
     "grade": false,
     "grade_id": "cell-1d3fd015fe8f64d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AiCVg5Ec8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a98218b35f47137eeba1ba1aead0700",
     "grade": false,
     "grade_id": "cell-a209b293a8850a57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.387394Z",
     "start_time": "2025-05-18T13:17:03.383475Z"
    },
    "deletable": false,
    "editable": false,
    "id": "Wug0q2Me8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb3bfcd4d49b2f2672447d8c65b6cb05",
     "grade": false,
     "grade_id": "cell-e246cbd9689e1a6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = model(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hFBSiQqS8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb863e279db4b170c35d8d0c7a37a1f",
     "grade": false,
     "grade_id": "cell-05ce9b9275e0d1cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.404884Z",
     "start_time": "2025-05-18T13:17:03.401375Z"
    },
    "deletable": false,
    "editable": false,
    "id": "vIzUAL-a8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16d7b53f211a6a1bef71c1dd2d1271cf",
     "grade": false,
     "grade_id": "cell-cb78afef7022f9a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {math.floor(s)}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:17:03.431675Z",
     "start_time": "2025-05-18T13:17:03.428812Z"
    },
    "deletable": false,
    "editable": false,
    "id": "pAlXhasn8qLv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b368f1767ddd0eddca44249fa47ed32",
     "grade": true,
     "grade_id": "cell-98f46bec0b8c87cc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0Xva5o5I8qLv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98abd7dd7805753c2e7b635f1265cb73",
     "grade": false,
     "grade_id": "cell-baf25642209867dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:24:27.841067Z",
     "start_time": "2025-05-18T13:17:03.474592Z"
    },
    "deletable": false,
    "editable": false,
    "id": "pRLHP-UQ8qLw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab44452ad9f838e0b56e1fc154ab6125",
     "grade": false,
     "grade_id": "cell-4900f92ae503be69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time elapsed: 0m 23s  ;  epochs: 100 (5.0%)  ;  loss: 2.229]\n",
      "Whe sherd asa asst is ma thes that as say hate lol ow sis at doldls hin, withy bond ids ford be thy his breat loust:\n",
      "Whalt salle, ar pulll apllt nont fis all lomy rerat?\n",
      "\n",
      "RKENGLOL:\n",
      "Hom chat he,\n",
      "Whe ald  \n",
      "\n",
      "[time elapsed: 0m 44s  ;  epochs: 200 (10.0%)  ;  loss: 1.983]\n",
      "Who wircem;\n",
      "Or the, his hi the 'gath the his haal shat rand, for to tell in: mar preang in greter May,\n",
      "On in wit mey that be frion thal'd thing. Gow me reteve shat, fon, were non hathe fors' and and pro \n",
      "\n",
      "[time elapsed: 1m 5s  ;  epochs: 300 (15.0%)  ;  loss: 2.01]\n",
      "Whe no'r to the ther swalours stich an there lilsends to here hooles.\n",
      "\n",
      "PROUCHETRO:\n",
      "It my mory wit'-genstore coull trowed to my this hand to o; the he blair.\n",
      "\n",
      "HPRICKI:\n",
      "It the mare to mad you, decorlmore\n",
      " \n",
      "\n",
      "[time elapsed: 1m 26s  ;  epochs: 400 (20.0%)  ;  loss: 2.072]\n",
      "Whersence for it forser an all to shands then dour aal\n",
      "He fave pist this beot be-can mes,\n",
      "And the and reap chars thest death,\n",
      "Nour crand and in the sie,\n",
      "The deed, as my balt.\n",
      "\n",
      "GRARELEL:\n",
      "And the wash tel \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     14\u001B[39m loss_avg = \u001B[32m0\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, n_epochs + \u001B[32m1\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     loss = train(*random_training_set())\n\u001B[32m     18\u001B[39m     loss_avg += loss\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m epoch % print_every == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(inp, target)\u001B[39m\n\u001B[32m      7\u001B[39m     output, hidden = model(inp[c], hidden)\n\u001B[32m      8\u001B[39m     loss += criterion(output, target[c].view(-\u001B[32m1\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m loss.backward()\n\u001B[32m     11\u001B[39m optimizer.step()\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.item() / chunk_len\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Miniconda3\\envs\\nlp-hw2\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m torch.autograd.backward(\n\u001B[32m    649\u001B[39m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001B[32m    650\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Miniconda3\\envs\\nlp-hw2\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m _engine_run_backward(\n\u001B[32m    354\u001B[39m     tensors,\n\u001B[32m    355\u001B[39m     grad_tensors_,\n\u001B[32m    356\u001B[39m     retain_graph,\n\u001B[32m    357\u001B[39m     create_graph,\n\u001B[32m    358\u001B[39m     inputs,\n\u001B[32m    359\u001B[39m     allow_unreachable=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    360\u001B[39m     accumulate_grad=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    361\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Miniconda3\\envs\\nlp-hw2\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable._execution_engine.run_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    825\u001B[39m         t_outputs, *args, **kwargs\n\u001B[32m    826\u001B[39m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100  # (D_h from the handout)\n",
    "num_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "model = OurModel(n_characters, hidden_size, n_characters, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'[time elapsed: {time_since(start)}  ;  epochs: {epoch} ({epoch / n_epochs * 100}%)  ;  loss: {loss:.4}]')\n",
    "        print(evaluate('Wh', 200), '\\n')  # generate text starting with 'Wh'\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "\n",
    "import numpy as np\n",
    "np.save('all_losses.npy', np.array(all_losses))\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"stoi\": {ch: i for i, ch in enumerate(all_characters)}\n",
    "    },\n",
    "    \"models/char_rnn_model.pth\"\n",
    ")\n",
    "print(\"Char-RNN weights & vocab saved to models/char_rnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dXeVkk298qLw",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8584d3be75d90a5197e7133411e0021d",
     "grade": false,
     "grade_id": "cell-ff9d72dafefa0a23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training Loss\n",
    "\n",
    "Plotting the the losses that were computed during training can provide a further indication that the network was indeed learning (Add your plot to the final submission pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-18T13:24:28.289927Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "#\n",
    "# plt.plot(all_losses)\n",
    "# plt.grid(True)\n",
    "# plt.xlabel('# of epochs (divided by plot_every)')\n",
    "# plt.ylabel('average loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:31:50.975470Z",
     "start_time": "2025-05-18T13:31:50.883668Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The cell you provided crashes my kernel constantly.\n",
    "I decided to save the loss list and plot it myself.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_losses = np.load('all_losses.npy')\n",
    "\n",
    "plt.plot(all_losses)\n",
    "plt.grid(True)\n",
    "plt.xlabel('# of epochs (divided by plot_every)')\n",
    "plt.ylabel('average loss')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
